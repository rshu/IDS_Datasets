\section{Existing IDS Dataset}


\subsection{ADFA Linux Dataset (ADFA-LD12)}

\textbf{Cited papers:} ~\cite{creech2014semantic} ~\cite{creech2013generation}~\cite{xie2014evaluating} \newline
\textbf{Description:} This dataset is generated based on Ubuntu Linux version 11.04, which runs a Linux web server and services with known vulnerabilities. It provides some services such as file sharing, database services, remote access and web server. This dataset recorded  the traces under attacks as described in the following table. \newline

\begin{table}[h]
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Payload/Effect}} & \multicolumn{1}{c|}{\textbf{Attack Vector}} \\ \hline
Password bruteforce & FTP by Hydra \\ \hline
Password bruteforce & SSH by Hydra \\ \hline
Add new superuser & Client side poisoned executable \\ \hline
Java Base Meterpreter & Tiki Wiki vulnerability exploit \\ \hline
Linux Meterpreter payload & Client side poisoned executable \\ \hline
C100 Webshell & \begin{tabular}[c]{@{}l@{}}PHP Remote File Inclusion \\ vulnerability\end{tabular} \\ \hline
\end{tabular}
\caption{Attack Structure}
\end{table}

\textbf{Research Goals:} ADFA-LD12 is designed for anomaly based systems, not for signature recognition IDS. Compared with old datasets such as KDD 98 and KDD 99, this dataset is much more representative of current attacks, and forms a realistic  and relevant metric for IDS performance metrics. Authors in the papers proposed a  new host-based anomaly detection method using discontiguous system call pattern on the ADFA dataset in an attempt to increase detection rate while reducing false alarm rates.\newline
\textbf{Data Structure:} This dataset contains three different data groups, and each group contains raw system call traces. Each training or validation data trace  was collected during normal operation of the host. Traces are generated using the auditd Unix program and then filtered by size. The training data has a size  with range 300 Bytes and 6KB, while the validation data keeps a size between 300 Bytes and 10KB. However, we could not get more details to infer the attributes from the description of the paper.\newline

\begin{table}[h]
\begin{tabular}{|l|c|}
\hline
\multicolumn{1}{|c|}{\textbf{Data Type}} & \textbf{Trace Count} \\ \hline
Normal Training Data & 833 Traces \\ \hline
Normal Validation Data & 4373 Traces \\ \hline
Attack Data & 100 Attacks per Vector \\ \hline
\end{tabular}
\caption{Dataset Structure}
\end{table}

\textbf{Evaluation Measurements:} The validity of the data set was examined by evaluating the performance of several IDS algorithms, i.e., hidden Markov models, the STIDE approach, K-Means clustering, and the K-Nearest Neighbour algorithm, and proposed semantic based methods. The result shows that the proposed method achieves a higher detection rate with a lower false alarm rate when compared with other algorithms.\newline


\subsection{CDX 2009}
\textbf{Cited papers:} ~\cite{homoliak2013asnm} ~\cite{sangster2009toward}~\cite{chen2014human}\newline
\textbf{Description:} This dataset contains data captured by NSA, data captured outside of the West Point network border and snort intrusion prevension log. The CDX dataset increases the scale of the network by demonstrating attack attempts from a 30 person red team using IP addresses from a pool of over sixty-five thousand host addresses against workstations, network devices, internal web servers, domain name servers, email servers, and chat servers from the 9 different collegiate team networks.\newline
\textbf{Research Goals:} To create a network based detection system for online defense against zero-day buffer overflow attacks in the production environment~\cite{homoliak2013asnm}. \newline
\textbf{Data Structure:} This dataset is a labeled dataset, and TCP dump traces includes all simulated communications and snort logs include information about the occurances of intrusions. \newline
\textbf{Evaluation Measurements:} \newline


\subsection{KDD CUP 98\&99 Dataset}
\textbf{Cited papers:}~\cite{barddal2015sncstream}~\cite{tavallaee2009detailed}\newline
\textbf{Description:}This dataset is an updated version of the DARPA98,
by processing the tcpdump portion. It contains different attacks such as Neptune-DoS, pod-DoS, Smurf-DoS, and buffer-overflow. The KDD experiments are performed on a Solaris-based system to collect a wide range of data. System calls are generated by processing the BSM audit data. 
\textbf{Research Goals:} This dataset is usually used to evaluate new intrusion detection methods based on analyzing network traffic. \newline
\textbf{Data Structure:} The training dataset consists of approximately 4.9 million single connection vectors, each labeled as either normal or attack, containing 41 features per connection record. \newline
\textbf{Evaluation Measurements:} \newline




\subsection{DARPA Intrusion Detection Data Sets}
\textbf{Cited papers:}~\cite{brown2009analysis} ~\cite{mchugh2000testing}\newline
\textbf{Description:} The datasets contained labeled data generated by simulating network traffic for a medium size U.S. Air Force base. The DARPA data sets represent the traffic of a relatively small network of 33 live and simulated hosts interacting with a total of 12 external hosts.The dataset was constructed for network security analysis and exposed the issues associated with the artificial injection of attacks and benign traffic. This dataset includes email, browsing, FTP, Telnet, IRC, and SNMP activities. It contains attacks such as DoS, Guess password, Buffer overflow, remote FTP, Syn flood, Nmap, and Rootkit.\newline
\textbf{Research Goals:} The labeled DARPA datasets of 1998 and 1999 were the security community benchmark for testing intrusion.
detection systems. \newline
\textbf{Data Structure:} \newline
\textbf{Evaluation Measurements:} \newline





\subsection{Intrusion detection evaluation dataset (ISCXIDS2012)}
\textbf{Cited papers:} ~\cite{shiravi2012toward}\newline
\textbf{Description:} The UNB ISCX IDS 2012 dataset consists of labeled network traces, including full packet payloads in pcap format, which along with the relevant profiles are publicly available for researchers.  \newline
\textbf{Research Goals:}  The UNB ISCX 2012 Intrusion Detection Evaluation Data Set pocesses the following characteristics: Realistic network and traffic, labeled dataset, total interaction capture, complete capture, diverse intrusion scenarios.  \newline
\textbf{Data Structure:} The UNB ISCX 2012 intrusion detection evaluation dataset consists of the following 7 days of network activity (normal and malicious). The UNB ISCX IDS 2012 dataset consists of labeled network traces, including full packet payloads in pcap format, which along with the relevant profiles. \newline
\textbf{Evaluation Measurements:} \newline




\subsection{Intrusion Detection Evaluation Dataset (CICIDS2017)}
\textbf{Cited papers:}~\cite{sharafaldin2018toward} \newline
\textbf{Description:} CICIDS2017 dataset contains benign and the most up-to-date common attacks, which resembles the true real-world data (PCAPs). It also includes the results of the network traffic analysis using CICFlowMeter with labeled flows based on the time stamp, source and destination IPs, source and destination ports, protocols and attack (CSV files). \newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} The CICIDS2017 dataset consists of labeled network flows, including full packet payloads in pcap format, the corresponding profiles and the labeled flows and CSV files for machine and deep learning purpose are publicly available for researchers. The data capturing period started at 9 a.m., Monday, July 3, 2017 and ended at 5 p.m. on Friday July 7, 2017, for a total of 5 days. Monday is the normal day and only includes the benign traffic. The implemented attacks include Brute Force FTP, Brute Force SSH, DoS, Heartbleed, Web Attack, Infiltration, Botnet and DDoS. They have been executed both morning and afternoon on Tuesday, Wednesday, Thursday and Friday.\newline
\textbf{Evaluation Measurements:} \newline




\subsection{NSL-KDD dataset}
\textbf{Cited papers:}~\cite{tavallaee2009detailed} ~\cite{bhuyan2015towards}\newline
\textbf{Description:} NSL-KDD is a data set suggested to solve some of the inherent problems of the KDD'99 data set. The number of records in the NSL-KDD train and test sets are reasonable. This advantage makes it affordable to run the experiments on the complete set without the need to randomly select a small portion. Consequently, evaluation results of different research work will be consistent and comparable. \newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} The NSL-KDD dataset consists of two parts: (i) KDDTrain+ and (ii) KDDTest+. The KDDTrain+ part of the NSL-KDD dataset is used to train a detection method or system to detect network intrusions. It contains four classes of attacks and a normal class dataset. The KDDTest+ part of NSLKDD dataset is used for testing a detection method or a system
when it is evaluated for performance. It also contains the same classes of traffic present in the training set \newline
\textbf{Evaluation Measurements:} \newline





\subsection{VPN-nonVPN dataset (ISCXVPN2016)}
\textbf{Cited papers:} ~\cite{draper2016characterization}\newline
\textbf{Description:} This dataset captures a regular session and a session over VPN, therefore we have a total of 14 traffic categories: VOIP, VPN-VOIP, P2P, VPN-P2P, etc.\newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} The traffic was captured using Wireshark and tcpdump, generating a total amount of 28GB of data. For the VPN, they used an external VPN service provider and connected to it using OpenVPN (UDP mode). To generate SFTP and FTPS traffic they also used an external service provider and Filezilla as a client.\newline
\textbf{Evaluation Measurements:} \newline



\subsection{Botnet dataset}
\textbf{Cited papers:} ~\cite{beigi2014towards}\newline
\textbf{Description:} To merge these data traces in one unified data set they employed so called overlay methodology, one of the most popular methods for creating synthetic datasets. Malicious data is usually captured by honeypots or through infecting computers with a given bot binary in a controlled environment. Botnet traces can be merged with benign data by mapping malicious data to either machines existing in the home network or machines outside of the current network. Considering the wide range of IP addresses in the traces, we mapped botnet IPs to the hosts outside of the current network using Bit-Twist packet generator. Malicious and benign traffic were then replayed using TCPReplay and captured by TCPdump as a single dataset. \newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} The dataset is divided into training and test datasets that included 7 and 16 types of botnets, respectively. The training dataset is 5.3 GB in size of which 43.92\% is malicious and the reminder contains normal flows. Test dataset is 8.5 GB of which 44.97\% is malicious flows.\newline
\textbf{Evaluation Measurements:} \newline



\subsection{Android validation dataset}
\textbf{Cited papers:}~\cite{gonzalez2014droidkin} \newline
\textbf{Description:} For the validation dataset 72 unique Android apps were selected from different sources. They manually selected one sample from each family of the Android Malware Genome Project dataset, eight samples from the Android Malware Virus Share package, nine samples from the Virus Total and five samples from the official Google Play market. \newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} This data set which consist in 72 original apps from different origins, and 10 transformations on each app, resulting in 792 apps with different relationships between them. The transformations performed featured the following operations: insert junk code, insert junk files, replace icons, replace files, different aligns, replace strings and more.\newline
\textbf{Evaluation Measurements:} \newline




\subsection{Android Botnet dataset}
\textbf{Cited papers:}~\cite{kadir2015android} \newline
\textbf{Description:} To give a comprehensive evaluation of Android botnets, they gathered a large collection of Android botnet samples representing 14 botnet families. Their accumulated dataset combines some botnet samples from the Android Genome Malware project, malware security blog, VirusTotal and samples provided by well-known anti-malware vendor. \newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} Overall, their dataset includes 1929 samples spawning a period of 2010 (the first appearance of Android botnet) to 2014.\newline
\textbf{Evaluation Measurements:} \newline





\subsection{Tor-nonTor dataset (ISCXTor2016)}
\textbf{Cited papers:}~\cite{lashkari2017characterization} \newline
\textbf{Description:} To be sure about the quantity and diversity of this dataset in CIC, they defined a set of tasks to generate a representative dataset of real-world traffic. They created three users for the browser traffic collection and two users for the communication parts such as chat, mail, FTP, p2p, etc. For the non-Tor traffic they used previous benign traffic from VPN project and for the Tor traffic they used 7 traffic categories.\newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} The traffic was captured using Wireshark and tcpdump, generating a total of 22GB of data. They captured the outgoing traffic at the workstation and the gateway simultaneously, collecting a set of pairs of .pcap files: one regular traffic pcap (workstation) and one Tor traffic pcap (gateway) file. They labelled the captured traffic in two steps. First, we processed the .pcap files captured at the workstation: we extracted the flows, and they confirmed that the majority of traffic flows were generated by application X (Skype, ftps, etc.), the object of the traffic capture. Then, they labelled all flows from the Tor .pcap file as X.\newline
\textbf{Evaluation Measurements:} \newline




\subsection{CIC DoS dataset}
\textbf{Cited papers:}~\cite{jazi2017detecting} \newline
\textbf{Description:} They have set up a testbed environment with a victim web server running Apache Linux v.2.2.22, PHP5 and Drupal v.7 as a content management system. The attacks were selected to represent the most common types of application layer DoS. \newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} Generated application layer DoS attacks were intermixed with the attack-free traces from the ISCX-IDS dataset. We produced 4 types of attacks with different tools, obtaining 8 different application layer DoS attack traces. These attacks were directed towards 10 web servers in ISCX data set that have the top highest number of connections. The resulting set contains 24 h of network traffic with total size of 4.6 GB.\newline
\textbf{Evaluation Measurements:} \newline




\subsection{Android Adware and General Malware Dataset}
\textbf{Cited papers:} ~\cite{lashkari2017towards}\newline
\textbf{Description:} They installed the Android applications on the real device and captured its network traffic.\newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} The dataset is generated from 1900 applications with the following three categories: Adware (250 apps), General Malware (150 apps), and Benign (1500 apps). The dataset consists of the following: 1) .pcap files – the network traffic of both the malware and benign (20\% malware and 80\% benign);2).csv files - the list of extracted network traffic features generated by the CIC-flowmeter.\newline
\textbf{Evaluation Measurements:} \newline



\subsection{Android Malware Dataset (CICAndMal2017)}
\textbf{Cited papers:} ~\cite{shiravi2012toward}\newline
\textbf{Description:} They run both malware and benign applications on real smartphones to avoid runtime behavior modification of advanced malware samples that are able to detect the emulator environment. \newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} We collected more than 10,854 samples (4,354 malware and 6,500 benign) from several sources. We have collected over six thousand benign apps from Googleplay market published in 2015, 2016, 2017. We installed 5,000 of the collected samples (426 malware and 5,065 benign) on real devices. Our malware samples in the CICAndMal2017 dataset are classified into four categories: Adware, Ransomware, Scareware, SMS Malware. Our samples come from 42 unique malware families. \newline
\textbf{Evaluation Measurements:} \newline


\subsection{CSE-CIC-IDS2018 on AWS}
\textbf{Cited papers:} \newline
\textbf{Description:} In CSE-CIC-IDS2018 dataset, we use the notion of profiles to generate datasets in a systematic manner, which will contain detailed descriptions of intrusions and abstract distribution models for applications, protocols, or lower level network entities. These profiles can be used by agents or human operators to generate events on the network. Due to the abstract nature of the generated profiles, we can apply them to a diverse range of network protocols with different topologies. Profiles can be used together to generate a dataset for specific needs.\newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} The dataset has been organized per day. For each day, we recorded the raw data including the network traffic (Pcaps) and event logs (windows and Ubuntu event Logs) per machine. In features extraction process from the raw data, we used the CICFlowMeter-V3 and extracted more than 80 traffic features and saved them as a CSV file per machine. \newline
\textbf{Evaluation Measurements:} \newline



\subsection{CAIDA}
\textbf{Cited papers:}~\cite{shiravi2012toward}~\cite{bhuyan2015empirical} ~\cite{moore2006inferring} \newline
\textbf{Description:} This organization has three different datasets, the CAIDA OC48, which includes different types of data observed on an OC48 link in San Jose, the CAIDA DDOS, which includes one-hour DDoS attack traffic split of 5-minute pcap files, and the CAIDA Internet traces 2016, which is passive traffic traces from CAIDA’s Equinix-Chicago monitor on
the High-speed Internet backbone\newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} The CAIDA dataset contains 5 minutes (i.e., 300 s) of anonymized traffic obtained during a DDoS attack on August 4, 2007. These traffic traces store only attack traffic to the victim and response from the victim; non-attack traffic has been removed as much as possible. It is a high-rate attack if there are more than 10,000 packets per second over the network, with 1000 attack packets per second covering 60\% of the attack traffic. As a result, this
is low-rate attack traffic.\newline
\textbf{Evaluation Measurements:} \newline


\subsection{LBNL dataset}
\textbf{Cited papers:} ~\cite{ashfaq2008comparative}\newline
\textbf{Description:} The dataset is an enterprise traffic dataset collected at the edge router of the Lawrence Berkeley National Lab (LBNL). Attack traffic in this dataset mostly comprises high-rate background traffic and low-rate outgoing scans. Traffic in this dataset comprises packet-level incoming, outgoing and internallyrouted traffic streams at the LBNL edge routers.\newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} This dataset contains both background traffic and attack traffic. Attack traffic was isolated by identifying scans in the aggregate traffic traces. Scans were identified by flagging those hosts which unsuccessfully
probed more than 20 hosts, out of which 16 hosts were probed in ascending or descending order. Malicious traffic mostly comprises failed incoming TCP SYN requests. \newline
\textbf{Evaluation Measurements:} \newline



\subsection{Kyoto}
\textbf{Cited papers:} ~\cite{song2011statistical}~\cite{sato2012unknown}~\cite{chitrakar2012anomaly}\newline
\textbf{Description:} Kyoto2006+ dataset is obtained from a honeypot networks of Kyoto University. In the honeypot networks, several types of honeypots are deployed over 5 different networks which are inside and outside of Kyoto University. There are some different OS, network printers and home information appliances (e.g. TV, Video Recorder). Kyoto2006+ dataset deploys a mail server in the same network to collect for normal traffic.\newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} From traffic data of the network, Kyoto 2006+ dataset extracts 14 conventional features and 10 additional features for each session. The former 14 features are extracted based on KDDCup 1999 dataset that is widely used for performance evaluation in intrusion detection system. The latter 10 features are extracted for more effective investigation. For example, signature-based IDS alerts, Antivirus alerts, source IP address and port number, time the session was started and so on. \newline
\textbf{Evaluation Measurements:} \newline


\subsection{Twente}
\textbf{Cited papers:}~\cite{sperotto2009labeled} \newline
\textbf{Description:} This dataset includes three services such as OpenSSH, Apache web server and Proftp using auth/ident on port 113 and captured data from a honeypot network by Netflow. There is some simultaneous network traffic such as auth/ident, ICMP, and IRC traffic, which are not completely benign or malicious. There is some simultaneous network traffic such as auth/ident, ICMP, and IRC traffic, which are not completely benign or malicious. Moreover, this dataset contains some unknown and uncorrelated alerts traffic.\newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} The data collection resulted in a 24 GB dump file containing 155.2 million packets. \newline
\textbf{Evaluation Measurements:} \newline


\subsection{UMASS}
\textbf{Cited papers:}~\cite{nehinbe2011critical}~\cite{prusty2011forensic} \newline
\textbf{Description:} The dataset includes trace files, which are network packets, and some traces on wireless applications. It has been generated using a single TCP-based download request attack scenario.\newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} \newline
\textbf{Evaluation Measurements:} \newline



\subsection{Drebin Dataset}
\textbf{Cited papers:} ~\cite{arp2014drebin}~\cite{torrano2009self}\newline
\textbf{Description:} In particular, we have acquired an initial dataset of 131,611 applications comprising benign as well as malicious software. The samples have been collected in the period from August 2010 to October 2012. In detail, the dataset contains 96,150 applications from the GooglePlay Store, 19,545 applications from different alternative Chinese Markets, 2,810 applications from alternative Russian Markets and 13,106 samples from other sources, such as Android websites, malware forums and security blogs. Additionally, the dataset includes all samples from the Android Malware Genome Project. \newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} The final dataset contains 123,453 benign applications and 5,560 malware samples. \newline
\textbf{Evaluation Measurements:} \newline


\subsection{HTTP DATASET CSIC 2010}
\textbf{Cited papers:}~\cite{torrano2009self} \newline
\textbf{Description:}  The HTTP dataset CSIC 2010 contains the generated traffic targeted to an e- Commerce web application developed at our department. In this web application, users can buy items using a shopping cart and register by providing some personal information. As it is a web application in Spanish, the data set contains some Latin characters. \newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} The dataset is generated automatically and contains 36,000 normal requests and more than 25,000 anomalous requests. The HTTP requests are labeled as normal or anomalous and the dataset includes attacks such as SQL injection, buffer overflow, information gathering, files disclosure, CRLF injection, XSS, server side include, parameter tampering and so on.\newline
\textbf{Evaluation Measurements:} \newline



\subsection{AWID}
\textbf{Cited papers:} ~\cite{kolias2016intrusion}\newline
\textbf{Description:} This dataset is a publicly available collection of sets of data in easily distributed format, which contain real traces of both normal and intrusive 802.11 traffic. \newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} The AWID collection of datasets is comprised of two equal sets which defer merely on the labeling method (AWID-CLS, AWID-ATK). The first one is labeled according to the classification introduced in Section III-E (4 classes), while the latter follows a more detailed classification based on the actual attacks (16 classes). \newline
\textbf{Evaluation Measurements:} \newline

\subsection{Coburg Intrusion Detection Data Sets(CIDDS)}
\textbf{Cited papers:}~\cite{ring2017flow}~\cite{ring2017creation} \newline
\textbf{Description:} They emulate a small business environment which includes several clients and typical servers. Network traffic is generated by scripts which emulate typical user activities like surfing the web, writing emails, or printing documents on the clients. These scripts follow some guidelines to ensure that the user behaviour is as realistic as possible, also with respect to working hours and lunch breaks. The generated network traffic is recorded in unidirectional NetFlow format.
For generating malicious traffic, attacks like Denial of Service, Brute Force, and Port Scans are executed within
the network. Since origins, targets, and timestamps of executed attacks are known, labelling of recorded
NetFlow data is easily possible. For inclusion of actual traffic, which has its origin outside the OpenStack
environment, an external server with two services is deployed. This server has a public IP address and is
exposed to real and up-to-date attacks from the internet. \newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} Nearly 32 million flows were captured from which around 31 million flows were captured in the OpenStack environment. Overall, we exploited 92 attacks within the four weeks.\newline
\textbf{Evaluation Measurements:} \newline


\subsection{Comprehensive, Multi-Source Cyber-Security Events}
\textbf{Cited papers:}~\cite{akent-2015-enterprise-data}~\cite{pritom2017study} \newline
\textbf{Description:} This data set represents 58 consecutive days of de-identified event data collected from five sources within Los Alamos National Laboratory's corporate, internal computer network. The data sources include Windows-based authentication events from both individual computers and centralized Active Directory domain controller servers; process start and stop events from individual Windows computers; Domain Name Service (DNS) lookups as collected on internal DNS servers; network flow data as collected on at several key router locations; and a set of well-defined red teaming events that present bad behavior within the 58 days. \newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} In total, the data set is approximately 12 gigabytes compressed across the five data elements and presents 1,648,275,307 events in total for 12,425 users, 17,684 computers, and 62,974 processes.\newline
\textbf{Evaluation Measurements:} \newline

\subsection{User-Computer Authentication Associations in Time}
\textbf{Cited papers:}~\cite{hagberg2014connected} \newline
\textbf{Description:} This anonymized data set encompasses 9 continuous months and represents 708,304,516 successful authentication events from users to computers collected from the Los Alamos National Laboratory (LANL) enterprise network. \newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} Each authentication event is on a separate line in the form of "time,user,computer" and represents a successful authentication by a user to a computer at the given time. There are 11,362 users within the data set, and 22,284 computers. Timestamps, with a resolution of 1 second, start at an epoch 1 and all subsequent times are an offset from this epoch. The time frame of the actual data collection is not provided to enhance the anonymization of the data. The values are comma delimited.  The data is available both as as one single file with 708,304,516 text lines or 9 files each with 30 days of events. \newline
\textbf{Evaluation Measurements:} \newline

\subsection{Unified Host and Network Dataset}
\textbf{Cited papers:}~\cite{turcotte17}~\cite{turcotte2017unified} \newline
\textbf{Description:} The Unified Host and Network Dataset is a subset of network and computer (host) events collected from the Los Alamos National Laboratory enterprise network over the course of approximately 90 days. The host event logs originated from most enterprise computers running the Microsoft Windows operating system on Los Alamos National Laboratory's (LANL) enterprise network. The network event data originated from many of the internal enterprise routers within the LANL enterprise network. \newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} The raw network flow data consisted of NetFlow V9 records that were exported from the core network routers to a centralized collection server. While V9 records can contain many different fields, only
the following are considered: StartTime, EndTime, SrcIP, DstIP, Protocol, SrcPort, DstPort, Packets and
Bytes. All windows host log data records will contain several attributes like EventID, LogHost, Time, LogonType, etc.  \newline
\textbf{Evaluation Measurements:} \newline


\subsection{Stratosphere CTU-13}
\textbf{Cited papers:}~\cite{garcia2014empirical} \newline
\textbf{Description:}The CTU-13 is a dataset of botnet traffic that was captured in the CTU University, Czech Republic, in 2011. The goal of the dataset was to have a large capture of real botnet traffic mixed with normal traffic and background traffic. The CTU-13 dataset consists in thirteen captures (called scenarios) of different botnet samples. On each scenario we executed a specific malware, which used several protocols and performed different actions.\newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} After capturing the packets, the dataset was preprocessed and
converted to a common format for the detectionmethods. The format selected was the NetFlow file standard
which is considered the ad-hoc standard for network data
representation. The conversion from pcap files to NetFlow
files was done in two steps using the Argus software suiteThese final NetFlow files were composed of the following fields: Start Time, End Time, Duration, Source IP address, Source Port, Direction, Destination IP address, Destination Port, State, SToS, Total Packets
and Total Bytes. \newline
\textbf{Evaluation Measurements:} \newline


\subsection{Detecting Malicious URLs}
\textbf{Cited papers:}~\cite{ma2010exploiting}~\cite{ma2009identifying}~\cite{ma2009beyond} \newline
\textbf{Description:} For benign URLs, they used two data sources. One is the
DMOZ Open Directory Project, which is a directory whose entries are vetted manually by editors. The second source of benign URLs was the random URL selector for Yahoo’s directory. They also drew from two sources for URLs to malicious sites: PhishTank and Spamscatter. PhishTank is a blacklist of phishing URLs consisting of manually-verified user contributions. Spamscatter is a spam collection infrastructure from which we extract
URLs from the bodies of those messages.\newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} The data set consists of about 2.4 million URLs (examples) and 3.2 million features. \newline
\textbf{Evaluation Measurements:} \newline


\subsection{UNM Intrusion Detection Dataset}
\textbf{Cited papers:}~\cite{yao2006enhanced} \newline
\textbf{Description:} Each trace is the list of system calls issued by an lpr process from the beginning of its execution to the end. There are 182 different system calls in the dataset.\newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} It consists of 4, 298 normal traces and 1,001 intrusion traces.\newline
\textbf{Evaluation Measurements:} \newline


\subsection{Apache JIRA and Chromium Dataset}
\textbf{Cited papers:}~\cite{peters2018text}~\cite{ohira2015dataset} \newline
\textbf{Description:}They use a total of five projects: four from Apache JIRA and a subset of bug reports from the Chromium project. Table The Chromium dataset comes from the 2011 mining challenge of the Mining Software Repositories conference. \newline
\textbf{Research Goals:}  \newline
\textbf{Data Structure:} There are six kinds of high impact bugs reports in
the datasets Apache JIRA. These include, surprise, dormant, blocking, security, performance and breakage. In Chromnium project, security bugs are labelled as Bug-Security when they are submitted to the system. Each row in Apache JIRA project represents a bug report and the columns are features of the reports such as bug id, title, description, and date and time a report was submitted and fixed. Chromium's html files are converted into a single CSV file with the
column headers, id, date, report, and security.\newline
\textbf{Evaluation Measurements:} \newline

\include{table}